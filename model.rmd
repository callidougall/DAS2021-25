---
title: "GroupProject2_model"
author: "Yufeng Zhang"
date: "2021/7/24"
output: html_document
---

```{r libraries, echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(moderndive)
library(pastecs)
library(skimr)
library(kableExtra)
library(gridExtra)
library(dplyr)
library(knitr)
library(MASS)
library(GGally)
library(pROC)
library(olsrr)
```

``` {R data.processing}
set.seed(2021)
ikea <- read.csv("dataset25.csv")

ikea = data

n=nrow(ikea)
ind1 <- sample(c(1:n),round(3*n/4))
ind2 <- setdiff(c(1:n),c(ind1))
train_data <- ikea[ind1,]
test_data <- ikea[ind2,]
```

# Data Analysis {#sec:DA}

We began by fitting the first order full GLM model of the following form:

$$ \ln\left(\frac{p}{1-p}\right) = \hat{\alpha} +  
\hat{\beta}_{\mbox{other_colors}}\cdot\mathbb{I}_{\mbox{other_colors}}(x) +
\hat{\beta}_{\mbox{category}}\cdot\mathbb{I}_{\mbox{category}}(x)+
\widehat{\beta}_{depth}X_{depth_i} + \widehat{\beta}_{height}X_{height_i} + \widehat{\beta}_{width}X_{width_i}+ \widehat{\beta}_{dh}X_{dh_i}+ \widehat{\beta}_{dw}X_{dw_i}+ \widehat{\beta}_{hw}X_{hw_i}+ \widehat{\beta}_{dhw}X_{dhw_i}$$
where

* p is the probability of price being 1.
* $\widehat{ln(\frac{p}{1-p})}$ is the log-odds of the price being 1.
* $\hat{\alpha}$ is the intercept, the baseline level of log-odds.
* $\hat{\beta}$ is the slope coefficient associated with the exploratory variables.
* $X_i$ is the value of the corresponding explanatory variable for the $i^{th}$ observation 
* $X_{dh_i}$ is the value of the interaction of variables depth and height for the $i^{th}$ observation ,$X_{dw_i}$ and $X_{hw_i}$ have the same meaning 
* $\mathbb{I}_{\mbox{other_colors}}(x)$ is an indicator function such that
$$\mathbb{I}_{\mbox{other_colors}}(x)=\left\{
\begin{array}{ll}
1 ~~~ \mbox{if the color of} ~ x \mbox{th observation is other colors},\\
0 ~~~ \mbox{Otherwise}.\\
\end{array}
\right.$$ 
* $\mathbb{I}_{\mbox{category}}(x)$ is an indicator function such that
$$\mathbb{I}_{\mbox{category}}(x)=\left\{
\begin{array}{ll}
1 ~~~ \mbox{if the category of} ~ x \mbox{th observation is the selected one},\\
0 ~~~ \mbox{Otherwise}.\\
\end{array}
\right.$$.

Through the p-value in the following R code, we can see that some components are not significant in the model, for they are more than 0.05, so we use AIC method to select the "best" model.

``` {R fullmodel, echo=TRUE, eval=TRUE, out.width = '80%', fig.align = "center", fig.pos = 'H'}
full_model <- glm(price ~ depth*height*width+other_colors+category, data = train_data, family = binomial(link = "logit"))
summary(full_model)
```

Stepwise regression was undertaken to determine the best-fitting model based on AIC. We began with the full model as the initial model, then variables were systematically added or removed (i.e. both forward and backward selection) based on a defined criterion, the lower AIC. From the R output below we can see the model with the lowest AIC is the final model which has an AIC of 267, includes the variables depth, height, width and category.

```{r stepwise, echo = TRUE, eval= TRUE, warning=FALSE, message=FALSE}
step_model1 = stepAIC(full_model, direction = "both", k = 2)
summary(step_model1)
```
Therefore, the final model is:

$$ \ln\left(\frac{p}{1-p}\right) = \hat{\alpha} +  
\hat{\beta}_{\mbox{other_colors}}\cdot\mathbb{I}_{\mbox{other_colors}}(x) +
\hat{\beta}_{\mbox{category}}\cdot\mathbb{I}_{\mbox{category}}(x)+
\widehat{\beta}_{depth}X_{depth_i} + \widehat{\beta}_{height}X_{height_i} + \widehat{\beta}_{width}X_{width_i}$$

where

* p is the probability of price being 1.
* $\widehat{ln(\frac{p}{1-p})}$ is the log-odds of the price being 1.
* $\hat{\alpha}$ is the intercept, the baseline level of log-odds.
* $\hat{\beta}$ is the slope coefficient associated with the exploratory variables.
* $X_i$ is the value of the corresponding explanatory variable for the $i^{th}$ observation 
* $\mathbb{I}_{\mbox{category}}(x)$ is an indicator function such that
$$\mathbb{I}_{\mbox{category}}(x)=\left\{
\begin{array}{ll}
1 ~~~ \mbox{if the category of} ~ x \mbox{th observation is the selected one},\\
0 ~~~ \mbox{Otherwise}.\\
\end{array}
\right.$$.

Then we can use the test data to test the model whether it is good to use. We test the model through looking at the value of p in the log-odds. If it equals to 1 or 0, it means the prediction is right, otherwise the prediction is wrong.
The following 125 data shows the prediction result of the model. We can see that most prediction of the data is TRUE, and less FALSE, which means the prediction of the model is good.

```{r predict}
test_predict = predict(step_model1, test_data)
test_data$prob = exp(test_predict) / ( 1 + exp(test_predict))
test_data$predict = exp(test_predict) / ( 1 + exp(test_predict))
test_data$predict[test_data$prob >= 0.5] = 1
test_data$predict[test_data$prob < 0.5] = 0
test_data$predict == test_data$price
```

The rate of predicting value 0 well is 83.5%, which is very good to do the prediction.
The rate of predicting value 1 well is 70.0%, and the false prediction is 30%, which is good but not enough. 

```{r 2x2 table}
test_data$price = as.character(test_data$price)
test_data$predict = as.character(test_data$predict)

data1 = test_data[test_data$price == "0",]
data2 = test_data[test_data$price == "1",]

a = table(data1$price == data1$predict)
b = table(data2$price == data2$predict)

table = matrix(c(71,12,14,28), nrow=2)
table[1,] = table[1,] / (71+14)
table[2,] = table[2,] / (12+28)

rownames(table) = c("actual value 0","actual value 1")
colnames(table) = c("predict value 0","predict value 1")
kable(table)
```

Here we can see that the overall right classification rate is 0.792, which seems good enough  for the model to fit the data.

```{r rate}
classication_rate = (71+28)/(125)
classication_rate
```

Here we use ROC curve to show the performance of the predictions from our model in terms of the true positive and false positive rates.
The area under the curve is about 0.872, which is reasonable considering that we have used four of the predictors available in the data.

```{r roc}
roc(test_data$price, test_data$prob, plot=TRUE, print.thres=TRUE, print.auc=TRUE)
```











