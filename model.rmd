---
title: "GroupProject2_model"
author: "Yufeng Zhang"
date: "2021/7/24"
output: html_document
---

```{r libraries, echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(moderndive)
library(pastecs)
library(skimr)
library(kableExtra)
library(gridExtra)
library(dplyr)
library(knitr)
library(MASS)
library(GGally)
```

``` {R data.processing}
set.seed(2021)
ikea <- read.csv("dataset25.csv")

ikea$price <- cut(ikea$price, breaks = c(-Inf, 1000, Inf), labels = c("0", "1"))
ikea<-ikea %>%
  mutate(volume=depth*height*width)

n=nrow(ikea)
ind1 <- sample(c(1:n),round(3*n/4))
ind2 <- setdiff(c(1:n),c(ind1))
train_data <- ikea[ind1,]
test_data <- ikea[ind2,]
write.csv(train_data, file = "D:\\train_data.csv", row.names = FALSE)
write.csv(test_data, file = "D:\\test_data.csv", row.names = FALSE)
```

# Data Analysis {#sec:DA}

We began by fitting the first order full GLM model of the following form:

$$ \widehat{ln(\frac{p}{1-p})}=\hat{\alpha}+\hat{\beta}_{depth}X_{depth}+\hat{\beta}_{height}X_{height}+\hat{\beta}_{width}X_{width}+\hat{\beta}_{volume}X_{volume}$$
where

* p is the probability of price being 1.
* $\widehat{ln(\frac{p}{1-p})}$ is the log-odds of the price being 1.
* $\hat{\alpha}$ is the intercept, the baseline level of log-odds.
* $\hat{\beta}$ is the slope coefficient associated with the exploratory variables.
* $X_i$ is the value of the explanatory variable for the observation.

Figure below displays the confidence intervals of the coefficients in the full model. From this it is evident that the parameter of `volume` is close to zero, therefore the effects of it are likely insignificant.

``` {R fullmodel, echo=TRUE, eval=TRUE, out.width = '80%', fig.align = "center", fig.pos = 'H'}
full_model <- glm(price ~ depth+height+width+volume, data = train_data, 
                            family = binomial(link = "logit"))
ggcoef(full_model,vline_color = "red",
       vline_linetype = "solid",
       errorbar_color = "blue",
       errorbar_height = .25,exclude_intercept = TRUE)
```

Stepwise regression was undertaken to determine the best-fitting model based on AIC. We began with the full model as the initial model, then variables were systematically added or removed (i.e. both forward and backward selection) based on a defined criterion, the lower AIC. From the R output below we can see the model with the lowest AIC is the final model which has an AIC of 123.94 and includes the variables depth, height and width, which is unsurprising as we determined their insignificance in figure above.

```{r stepwise, echo = TRUE, eval= TRUE, warning=FALSE, message=FALSE}
step_model1 = stepAIC(full_model, direction = "both", k = 2)
```

The following code produces table 1, which shows the selected model and its corresponding parameter estimates. Notice that the variables are all significant at predicting the log-odds at the 5% significance level, as their p-values are all close to 0.

```{r newmodel, echo = TRUE, eval= TRUE, warning=FALSE, message=FALSE}
model1 <- glm(price ~ depth+height+width, data = train_data, 
                            family = binomial(link = "logit"))
summodel1=summary(model1)
kable(summodel1$coefficients) %>%
kable_styling(latex_options = 'HOLD_position')
```

Thus, the fitted MLR model has the following form:

$$ \widehat{ln(\frac{p}{1-p})}=-10.394+0.074 \cdot X_{depth}+0.022 \cdot X_{height}+0.027 \cdot X_{width}$$
The confidence intervals of the coefficients of the final model are shown below (figure 2).

``` {R ci_final_model}

ci <- ggcoef(model1, vline_color = "red", conf.level = 0.95,
       vline_linetype = "solid",
       errorbar_color = "blue",
       errorbar_height = .25,exclude_intercept = TRUE)

```

From the estimates above, increasing depth, height or width will increase the log-odds and therefore increase the probability of the price higher than 1000$. 

The following code produces figure ?, which displays a plot of the residuals against the fitted values. From this it is evident the residuals are evenly scattered around zero. Also, the spread of the residuals remains constant across the fitted values. Hence the model assumptions of the residuals having mean zero and constant variance appear valid. Also, figure ? shows a histogram of the residuals, it appears a normal pattern centered at zero. Hence, it is concluded that the assumptions of the MLR model hold, that is, the residual has mean zero, constant variance and normal distribution.

```{r residual, echo = TRUE, out.width = '80%', fig.align = "center", fig.pos = 'H'}

residual = as.vector(step_model1[[2]])
fitted_value = as.vector(step_model1[[3]])
dummy = cbind(residual,fitted_value)
dummy = as.data.frame(dummy)
ggplot(dummy, aes(x = fitted_value, y = residual)) +
  geom_point() +
  labs(x = "Fitted value", y = "Residual")  +
  geom_hline(yintercept = 0, col = "blue", size = 1)
```

```{r hist, echo = TRUE, out.width = '80%', fig.align = "center", fig.pos = 'H'}
ggplot(dummy, aes(x = residual)) +
  geom_histogram(color = "white") +
  labs(x = "Residual")
```

```{r all, echo = TRUE}
AIC = 123.94
R2 = summary(model1)[[7]]
test_data=na.omit(test_data)
model1_predict = predict(model1, test_data)
real_value=as.vector(test_data[4])
real_value=as.matrix(real_value)
real_value=as.vector(real_value)
real_value=as.numeric(real_value)
MSE1 = sum((real_value - model1_predict)^2) / nrow(test_data)

MSE_table = round(cbind(AIC,R2,MSE1), digit=3)
rownames(MSE_table) = "The fitted model"
colnames(MSE_table) = c("AIC", "Adjusted R-squared", "MSE")
kable(MSE_table) %>%
kable_styling(latex_options = "hold_position")
```
